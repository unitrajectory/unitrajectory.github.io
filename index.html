<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UniTrajectory: Unified Trajectory-guided Video Generation</title>
    <style>
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.7;
            background-color: #f8f9fa;
            color: #333;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 800px;
            margin: 40px auto;
            padding: 40px;
            background-color: #ffffff;
            border: 1px solid #dee2e6;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            border-radius: 8px;
        }
        h1 {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            font-size: 2em;
            color: #212529;
            text-align: center;
            margin-bottom: 0.5em;
        }
        .authors {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            font-size: 1.1em;
            text-align: center;
            color: #495057;
            margin-bottom: 2em;
        }
        h2 {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            font-size: 1.5em;
            color: #343a40;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
            margin-top: 2em;
            margin-bottom: 1em;
        }
        .abstract-content {
            text-align: justify;
            text-indent: 2em;
            color: #495057;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            font-size: 0.9em;
            color: #6c757d;
        }
    </style>
</head>
<body>

    <div class="container">
        <header>
            <h1>UniTrajectory: Unified Trajectory-guided Video Generation</h1>
            <p class="authors">Yuan Liu, Chenjie Yang, Yansong Liu, Xintao Hu, Jack Ma</p>
        </header>

        <main>
            <section id="abstract">
                <h2>Abstract</h2>
                <p class="abstract-content">
                    The dynamism of videos is their core appeal, and motion control serves as a pivotal enabler for dynamic creation and creative expression. However, existing text-to-video (T2V) and image-to-video (I2V) video generation models lack a unified motion control framework, failing to characterize precise camera movements and subtle object dynamics, while current trajectory control methods suffer from insufficient motion control granularity and are often plagued by visual distortion or trajectory deviation. To address these challenges, we propose UniTrajectory, a unified trajectory-guided video generation framework supporting I2V and video-to-video (V2V) generation with independent or compositional camera and object motion control. It integrates a user-friendly data pipeline for versatile control signal creation: camera signals from dynamic point clouds and adjustable paths, object signals from user-drawn trajectories or mask-based predefined motions. The method uses an effective training strategy: first, concatenating conditioning signal latent with input video latent and fine-tuning a pre-trained Diffusion Transformers (DiT)-based model via LoRA to avoid extra modules and heavy computational overhead; second, employing mask-based, density-aware point sampling during training to support sparse-to-dense control signals at inference. Ultimately, our framework empowers users with capabilities including dynamic camera control, object path guidance, and motion transfer. Comprehensive experiments validate that it generates high-quality videos with improved accuracy and adaptability in complex motion control, delivering precise and natural dynamic effects.
                </p>
            </section>
        </main>
        
        <footer>
            <p>&copy; 2025 Yuan Liu, et al.</p>
        </footer>
    </div>

</body>
</html>
